{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfd6npKoCwLw",
        "outputId": "26b603d3-3b58-425a-9295-583df6edc71e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/My Drive/PyProjects/LLMs\n",
            "eval_communist\tLLaMA-Factory  LLMs_demo.ipynb\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/My Drive/PyProjects/LLMs\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bTyIoNND4PK",
        "outputId": "d87303f9-24e0-42f6-8ddf-f2d3096815c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 5906, done.\u001b[K\n",
            "remote: Counting objects: 100% (1783/1783), done.\u001b[K\n",
            "remote: Compressing objects: 100% (156/156), done.\u001b[K\n",
            "remote: Total 5906 (delta 1678), reused 1638 (delta 1627), pack-reused 4123\u001b[K\n",
            "Receiving objects: 100% (5906/5906), 185.92 MiB | 10.79 MiB/s, done.\n",
            "Resolving deltas: 100% (4315/4315), done.\n",
            "Updating files: 100% (132/132), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYqU1sN9QMxM"
      },
      "outputs": [],
      "source": [
        "!chmod +x /content/drive/MyDrive/PyProjects/LLMs/Qwen-1_8B-Chat-Int8/.git/hooks/post-checkout\n",
        "# !git lfs install\n",
        "# !git clone https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqIp2-UCFozu",
        "outputId": "609daada-4dfe-4686-fb2c-a96a7a59bc28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/PyProjects/LLMs\n"
          ]
        }
      ],
      "source": [
        "%cd LLaMA-Factory\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKJ-ioB7F0mO"
      },
      "outputs": [],
      "source": [
        "!pip install kaleido\n",
        "!pip install cohere\n",
        "!pip install openai\n",
        "!pip install fastapi\n",
        "!pip install python-multipart\n",
        "!pip install uvicorn\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o6IuR8KJg6o"
      },
      "outputs": [],
      "source": [
        "!python src/web_demo.py \\\n",
        "    --model_name_or_path Qwen/Qwen-1_8B-Chat-Int8 \\\n",
        "    --adapter_name_or_path saves/Qwen-1.8B-int8-Chat/lora/communist \\\n",
        "    --template default \\\n",
        "    --finetuning_type lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0DFByEELVRw"
      },
      "outputs": [],
      "source": [
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "  --stage sft \\\n",
        "  --do_train True \\\n",
        "  --model_name_or_path /content/drive/MyDrive/PyProjects/LLMs/Qwen-1_8B-Chat-Int8 \\\n",
        "  --finetuning_type lora \\\n",
        "  --template qwen \\\n",
        "  --dataset_dir /content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/data \\\n",
        "  --dataset community \\\n",
        "  --cutoff_len 1024 \\\n",
        "  --learning_rate 5e-05 \\\n",
        "  --num_train_epochs 3.0 \\\n",
        "  --max_samples 2000 \\\n",
        "  --per_device_train_batch_size 4 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --lr_scheduler_type cosine \\\n",
        "  --max_grad_norm 1.0 \\\n",
        "  --logging_steps 10 \\\n",
        "  --save_steps 100 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --neftune_noise_alpha 0 \\\n",
        "  --lora_rank 8 \\\n",
        "  --lora_dropout 0.1 \\\n",
        "  --lora_target c_attn \\\n",
        "  --output_dir saves/Qwen-1.8B-int8-Chat/lora/communist \\\n",
        "  --fp16 True \\\n",
        "  --plot_loss True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXYNCkk0RWYq"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers==4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed\n",
        "!pip install transformers_stream_generator einops\n",
        "!pip install auto-gptq optimum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm8f5GFeOZzY"
      },
      "outputs": [],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python src/train_web.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lfY1gAJVXF1C"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"2209048577@qq.com\"\n",
        "!git config --global user.name \"Hurricane-hub\"\n",
        "!git config --global user.password \"whk.0116.com\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!Get-FileHash -Path \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/data/community.json\" -Algorithm SHA1\n",
        "#!pip install flash_attn\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40c1ppRmXigF",
        "outputId": "4b93cb68-960f-44a5-b049-e978d97fa6e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval_communist\tLLaMA-Factory  LLMs_demo.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init\n",
        "!git add .\n",
        "!git commit -m \"first commit\"\n",
        "!git remote add origin https://github.com/Hurricane-hub/NEU_NLP.git\n",
        "!git remote set-url origin https://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@github.com/Hurricane-hub/NEU_NLP.git\n",
        "!git pull origin master\n",
        "\n",
        "!git push -u origin master"
      ],
      "metadata": {
        "id": "atLN8cSdsqWO",
        "outputId": "9c457db3-ec0e-4f07-8b24-81cefaa227e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: couldn't find remote ref master\n",
            "Enumerating objects: 171, done.\n",
            "Counting objects: 100% (171/171), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (157/157), done.\n",
            "Writing objects: 100% (171/171), 79.55 MiB | 4.28 MiB/s, done.\n",
            "Total 171 (delta 12), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (12/12), done.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: File LLaMA-Factory/data/oaast_rm.json is 53.48 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "remote: \n",
            "remote: Create a pull request for 'master' on GitHub by visiting:\u001b[K\n",
            "remote:      https://github.com/Hurricane-hub/NEU_NLP/pull/new/master\u001b[K\n",
            "remote: \n",
            "To https://github.com/Hurricane-hub/NEU_NLP.git\n",
            " * [new branch]      master -> master\n",
            "Branch 'master' set up to track remote branch 'master' from 'origin'.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA8KbwFc273W34P5/feuEy"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}