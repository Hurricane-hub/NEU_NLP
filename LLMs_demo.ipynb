{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46571,"status":"ok","timestamp":1704792096701,"user":{"displayName":"王宁","userId":"09376064211353228652"},"user_tz":-480},"id":"Nfd6npKoCwLw","outputId":"7e69255b-7ac5-4fca-ab3d-d9a82b2b878d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n","/content/drive/My Drive/PyProjects/LLMs\n","eval_communist\tLLaMA-Factory  LLMs_demo.ipynb\t__pycache__  Qwen-1_8B-Chat-Int8\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd /content/drive/My Drive/PyProjects/LLMs\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27931,"status":"ok","timestamp":1704331706197,"user":{"displayName":"王宁","userId":"09376064211353228652"},"user_tz":-480},"id":"-bTyIoNND4PK","outputId":"d87303f9-24e0-42f6-8ddf-f2d3096815c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'LLaMA-Factory'...\n","remote: Enumerating objects: 5906, done.\u001b[K\n","remote: Counting objects: 100% (1783/1783), done.\u001b[K\n","remote: Compressing objects: 100% (156/156), done.\u001b[K\n","remote: Total 5906 (delta 1678), reused 1638 (delta 1627), pack-reused 4123\u001b[K\n","Receiving objects: 100% (5906/5906), 185.92 MiB | 10.79 MiB/s, done.\n","Resolving deltas: 100% (4315/4315), done.\n","Updating files: 100% (132/132), done.\n"]}],"source":["!git clone https://github.com/hiyouga/LLaMA-Factory.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYqU1sN9QMxM"},"outputs":[],"source":["!chmod +x /content/drive/MyDrive/PyProjects/LLMs/Qwen-1_8B-Chat-Int8/.git/hooks/post-checkout\n","# !git lfs install\n","# !git clone https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1704718645811,"user":{"displayName":"王宁","userId":"09376064211353228652"},"user_tz":-480},"id":"sqIp2-UCFozu"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory\n","assets\tCODE_OF_CONDUCT.md  evaluation\tpyproject.toml\tREADME_zh.md\t  saves     src\n","cache\tdata\t\t    LICENSE\tREADME.md\trequirements.txt  setup.py  tests\n"]}],"source":["%cd LLaMA-Factory\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20157,"status":"ok","timestamp":1704718665957,"user":{"displayName":"王宁","userId":"09376064211353228652"},"user_tz":-480},"id":"W0TOIrbSG86I","outputId":"46f5d89d-1b04-4f50-8991-bca0db03ae8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting fastapi\n","  Downloading fastapi-0.108.0-py3-none-any.whl (92 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/92.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m81.9/92.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,\u003c3.0.0,\u003e=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.13)\n","Collecting starlette\u003c0.33.0,\u003e=0.29.0 (from fastapi)\n","  Downloading starlette-0.32.0.post1-py3-none-any.whl (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions\u003e=4.8.0 (from fastapi)\n","  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n","Requirement already satisfied: anyio\u003c5,\u003e=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette\u003c0.33.0,\u003e=0.29.0-\u003efastapi) (3.7.1)\n","Requirement already satisfied: idna\u003e=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5,\u003e=3.4.0-\u003estarlette\u003c0.33.0,\u003e=0.29.0-\u003efastapi) (3.6)\n","Requirement already satisfied: sniffio\u003e=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5,\u003e=3.4.0-\u003estarlette\u003c0.33.0,\u003e=0.29.0-\u003efastapi) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5,\u003e=3.4.0-\u003estarlette\u003c0.33.0,\u003e=0.29.0-\u003efastapi) (1.2.0)\n","Installing collected packages: typing-extensions, starlette, fastapi\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions\u003c4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed fastapi-0.108.0 starlette-0.32.0.post1 typing-extensions-4.9.0\n","Collecting python-multipart\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-multipart\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed python-multipart-0.0.6\n","Collecting uvicorn\n","  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click\u003e=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n","Collecting h11\u003e=0.8 (from uvicorn)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions\u003e=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.9.0)\n","Installing collected packages: h11, uvicorn\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h11-0.14.0 uvicorn-0.25.0\n"]}],"source":["!pip install fastapi\n","!pip install python-multipart\n","!pip install uvicorn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65311,"status":"ok","timestamp":1704718731253,"user":{"displayName":"王宁","userId":"09376064211353228652"},"user_tz":-480},"id":"TKJ-ioB7F0mO","outputId":"4b12fa43-06d4-4b07-dee1-5f303f8f564d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting kaleido\n","  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: kaleido\n","Successfully installed kaleido-0.2.1\n","Collecting cohere\n","  Downloading cohere-4.40-py3-none-any.whl (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp\u003c4.0,\u003e=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.1)\n","Collecting backoff\u003c3.0,\u003e=2.0 (from cohere)\n","  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Collecting fastavro\u003c2.0,\u003e=1.8 (from cohere)\n","  Downloading fastavro-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting importlib_metadata\u003c7.0,\u003e=6.0 (from cohere)\n","  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003c4.0,\u003e=3.0-\u003ecohere) (23.1.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003c4.0,\u003e=3.0-\u003ecohere) (6.0.4)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003c4.0,\u003e=3.0-\u003ecohere) (1.9.4)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003c4.0,\u003e=3.0-\u003ecohere) (1.4.1)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003c4.0,\u003e=3.0-\u003ecohere) (1.3.1)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003c4.0,\u003e=3.0-\u003ecohere) (4.0.3)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata\u003c7.0,\u003e=6.0-\u003ecohere) (3.17.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.25.0-\u003ecohere) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.25.0-\u003ecohere) (3.6)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.25.0-\u003ecohere) (2023.11.17)\n","Installing collected packages: importlib_metadata, fastavro, backoff, cohere\n","  Attempting uninstall: importlib_metadata\n","    Found existing installation: importlib-metadata 7.0.0\n","    Uninstalling importlib-metadata-7.0.0:\n","      Successfully uninstalled importlib-metadata-7.0.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed backoff-2.2.1 cohere-4.40 fastavro-1.9.2 importlib_metadata-6.11.0\n","Collecting openai\n","  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio\u003c5,\u003e=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro\u003c2,\u003e=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx\u003c1,\u003e=0.23.0 (from openai)\n","  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic\u003c3,\u003e=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n","Requirement already satisfied: tqdm\u003e4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n","Requirement already satisfied: typing-extensions\u003c5,\u003e=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: idna\u003e=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5,\u003e=3.5.0-\u003eopenai) (3.6)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5,\u003e=3.5.0-\u003eopenai) (1.2.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx\u003c1,\u003e=0.23.0-\u003eopenai) (2023.11.17)\n","Collecting httpcore==1.* (from httpx\u003c1,\u003e=0.23.0-\u003eopenai)\n","  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: h11\u003c0.15,\u003e=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*-\u003ehttpx\u003c1,\u003e=0.23.0-\u003eopenai) (0.14.0)\n","Installing collected packages: httpcore, httpx, openai\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed httpcore-1.0.2 httpx-0.26.0 openai-1.6.1\n","Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.108.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,\u003c3.0.0,\u003e=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.13)\n","Requirement already satisfied: starlette\u003c0.33.0,\u003e=0.29.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.32.0.post1)\n","Requirement already satisfied: typing-extensions\u003e=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.9.0)\n","Requirement already satisfied: anyio\u003c5,\u003e=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette\u003c0.33.0,\u003e=0.29.0-\u003efastapi) (3.7.1)\n","Requirement already satisfied: idna\u003e=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5,\u003e=3.4.0-\u003estarlette\u003c0.33.0,\u003e=0.29.0-\u003efastapi) (3.6)\n","Requirement already satisfied: sniffio\u003e=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5,\u003e=3.4.0-\u003estarlette\u003c0.33.0,\u003e=0.29.0-\u003efastapi) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5,\u003e=3.4.0-\u003estarlette\u003c0.33.0,\u003e=0.29.0-\u003efastapi) (1.2.0)\n","Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (0.0.6)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.25.0)\n","Requirement already satisfied: click\u003e=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n","Requirement already satisfied: h11\u003e=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n","Requirement already satisfied: typing-extensions\u003e=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.9.0)\n","Requirement already satisfied: torch\u003e=1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu121)\n","Collecting transformers\u003e=4.36.2 (from -r requirements.txt (line 2))\n","  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\u003e=2.14.3 (from -r requirements.txt (line 3))\n","  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate\u003e=0.21.0 (from -r requirements.txt (line 4))\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting peft\u003e=0.7.0 (from -r requirements.txt (line 5))\n","  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trl\u003e=0.7.6 (from -r requirements.txt (line 6))\n","  Downloading trl-0.7.7-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gradio\u003c4.0.0,\u003e=3.38.0 (from -r requirements.txt (line 7))\n","  Downloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.11.4)\n","Collecting sentencepiece (from -r requirements.txt (line 9))\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.20.3)\n","Collecting tiktoken (from -r requirements.txt (line 11))\n","  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.42.1)\n","Collecting rouge-chinese (from -r requirements.txt (line 13))\n","  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (3.8.1)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (0.25.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (1.10.13)\n","Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (0.108.0)\n","Collecting sse-starlette (from -r requirements.txt (line 18))\n","  Downloading sse_starlette-1.8.2-py3-none-any.whl (8.9 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (3.7.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.1-\u003e-r requirements.txt (line 1)) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.1-\u003e-r requirements.txt (line 1)) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.1-\u003e-r requirements.txt (line 1)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.1-\u003e-r requirements.txt (line 1)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.1-\u003e-r requirements.txt (line 1)) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.1-\u003e-r requirements.txt (line 1)) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.1-\u003e-r requirements.txt (line 1)) (2.1.0)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (0.20.1)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (1.23.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (23.2)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (2.31.0)\n","Requirement already satisfied: tokenizers\u003c0.19,\u003e=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (0.15.0)\n","Requirement already satisfied: safetensors\u003e=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (0.4.1)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (4.66.1)\n","Requirement already satisfied: pyarrow\u003e=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets\u003e=2.14.3-\u003e-r requirements.txt (line 3)) (10.0.1)\n","Collecting pyarrow-hotfix (from datasets\u003e=2.14.3-\u003e-r requirements.txt (line 3))\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill\u003c0.3.8,\u003e=0.3.0 (from datasets\u003e=2.14.3-\u003e-r requirements.txt (line 3))\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets\u003e=2.14.3-\u003e-r requirements.txt (line 3)) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets\u003e=2.14.3-\u003e-r requirements.txt (line 3)) (3.4.1)\n","Collecting multiprocess (from datasets\u003e=2.14.3-\u003e-r requirements.txt (line 3))\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets\u003e=2.14.3-\u003e-r requirements.txt (line 3)) (3.9.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate\u003e=0.21.0-\u003e-r requirements.txt (line 4)) (5.9.5)\n","Collecting tyro\u003e=0.5.11 (from trl\u003e=0.7.6-\u003e-r requirements.txt (line 6))\n","  Downloading tyro-0.6.3-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.9/78.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiofiles\u003c24.0,\u003e=22.0 (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7))\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair\u003c6.0,\u003e=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (4.2.2)\n","Collecting ffmpy (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7))\n","  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.6.1 (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7))\n","  Downloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (0.26.0)\n","Requirement already satisfied: importlib-resources\u003c7.0,\u003e=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (6.1.1)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (2.1.3)\n","Collecting orjson~=3.0 (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7))\n","  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow\u003c11.0,\u003e=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (9.4.0)\n","Collecting pydub (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7))\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (0.0.6)\n","Collecting semantic-version~=2.0 (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7))\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting websockets\u003c12.0,\u003e=10.0 (from gradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7))\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese-\u003e-r requirements.txt (line 13)) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk-\u003e-r requirements.txt (line 14)) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk-\u003e-r requirements.txt (line 14)) (1.3.2)\n","Requirement already satisfied: h11\u003e=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn-\u003e-r requirements.txt (line 15)) (0.14.0)\n","Requirement already satisfied: starlette\u003c0.33.0,\u003e=0.29.0 in /usr/local/lib/python3.10/dist-packages (from fastapi-\u003e-r requirements.txt (line 17)) (0.32.0.post1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette-\u003e-r requirements.txt (line 18)) (3.7.1)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003e-r requirements.txt (line 19)) (1.2.0)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003e-r requirements.txt (line 19)) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003e-r requirements.txt (line 19)) (4.47.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003e-r requirements.txt (line 19)) (1.4.5)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003e-r requirements.txt (line 19)) (3.1.1)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003e-r requirements.txt (line 19)) (2.8.2)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair\u003c6.0,\u003e=4.2.0-\u003egradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (0.4)\n","Requirement already satisfied: jsonschema\u003e=3.0 in /usr/local/lib/python3.10/dist-packages (from altair\u003c6.0,\u003e=4.2.0-\u003egradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair\u003c6.0,\u003e=4.2.0-\u003egradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (0.12.0)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets\u003e=2.14.3-\u003e-r requirements.txt (line 3)) (23.1.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets\u003e=2.14.3-\u003e-r requirements.txt (line 3)) (6.0.4)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets\u003e=2.14.3-\u003e-r requirements.txt (line 3)) (1.9.4)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets\u003e=2.14.3-\u003e-r requirements.txt (line 3)) (1.4.1)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets\u003e=2.14.3-\u003e-r requirements.txt (line 3)) (1.3.1)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets\u003e=2.14.3-\u003e-r requirements.txt (line 3)) (4.0.3)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets\u003e=2.14.3-\u003e-r requirements.txt (line 3)) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (3.6)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers\u003e=4.36.2-\u003e-r requirements.txt (line 2)) (2023.11.17)\n","Requirement already satisfied: sniffio\u003e=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio-\u003esse-starlette-\u003e-r requirements.txt (line 18)) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio-\u003esse-starlette-\u003e-r requirements.txt (line 18)) (1.2.0)\n","Collecting docstring-parser\u003e=0.14.1 (from tyro\u003e=0.5.11-\u003etrl\u003e=0.7.6-\u003e-r requirements.txt (line 6))\n","  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n","Requirement already satisfied: rich\u003e=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro\u003e=0.5.11-\u003etrl\u003e=0.7.6-\u003e-r requirements.txt (line 6)) (13.7.0)\n","Collecting shtab\u003e=1.5.6 (from tyro\u003e=0.5.11-\u003etrl\u003e=0.7.6-\u003e-r requirements.txt (line 6))\n","  Downloading shtab-1.6.5-py3-none-any.whl (13 kB)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx-\u003egradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (1.0.2)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.13.1-\u003e-r requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: jsonschema-specifications\u003e=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema\u003e=3.0-\u003ealtair\u003c6.0,\u003e=4.2.0-\u003egradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (2023.11.2)\n","Requirement already satisfied: referencing\u003e=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema\u003e=3.0-\u003ealtair\u003c6.0,\u003e=4.2.0-\u003egradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (0.32.0)\n","Requirement already satisfied: rpds-py\u003e=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema\u003e=3.0-\u003ealtair\u003c6.0,\u003e=4.2.0-\u003egradio\u003c4.0.0,\u003e=3.38.0-\u003e-r requirements.txt (line 7)) (0.15.2)\n","Requirement already satisfied: markdown-it-py\u003e=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=11.1.0-\u003etyro\u003e=0.5.11-\u003etrl\u003e=0.7.6-\u003e-r requirements.txt (line 6)) (3.0.0)\n","Requirement already satisfied: pygments\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=11.1.0-\u003etyro\u003e=0.5.11-\u003etrl\u003e=0.7.6-\u003e-r requirements.txt (line 6)) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py\u003e=2.2.0-\u003erich\u003e=11.1.0-\u003etyro\u003e=0.5.11-\u003etrl\u003e=0.7.6-\u003e-r requirements.txt (line 6)) (0.1.2)\n","Building wheels for collected packages: ffmpy\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=6758044a00a46fd3d019651470466038c3e78bbd7846f7967042603547249923\n","  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n","Successfully built ffmpy\n","Installing collected packages: sentencepiece, pydub, ffmpy, websockets, shtab, semantic-version, rouge-chinese, pyarrow-hotfix, orjson, docstring-parser, dill, aiofiles, tiktoken, multiprocess, tyro, gradio-client, accelerate, transformers, sse-starlette, datasets, trl, peft, gradio\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.2\n","    Uninstalling transformers-4.35.2:\n","      Successfully uninstalled transformers-4.35.2\n","Successfully installed accelerate-0.25.0 aiofiles-23.2.1 datasets-2.16.1 dill-0.3.7 docstring-parser-0.15 ffmpy-0.3.1 gradio-3.50.2 gradio-client-0.6.1 multiprocess-0.70.15 orjson-3.9.10 peft-0.7.1 pyarrow-hotfix-0.6 pydub-0.25.1 rouge-chinese-1.0.3 semantic-version-2.10.0 sentencepiece-0.1.99 shtab-1.6.5 sse-starlette-1.8.2 tiktoken-0.5.2 transformers-4.36.2 trl-0.7.7 tyro-0.6.3 websockets-11.0.3\n"]}],"source":["!pip install kaleido\n","!pip install cohere\n","!pip install openai\n","!pip install fastapi\n","!pip install python-multipart\n","!pip install uvicorn\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55546,"status":"ok","timestamp":1704445298013,"user":{"displayName":"王宁","userId":"09376064211353228652"},"user_tz":-480},"id":"1o6IuR8KJg6o","outputId":"28683224-f118-43d4-a8b1-57484e33d477"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-01-05 09:01:01.729412: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-05 09:01:01.729466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-05 09:01:01.730821: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-05 09:01:02.902536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","tokenizer_config.json: 100% 173/173 [00:00\u003c00:00, 834kB/s]\n","tokenization_qwen.py: 100% 9.62k/9.62k [00:00\u003c00:00, 28.4MB/s]\n","qwen.tiktoken: 100% 2.56M/2.56M [00:00\u003c00:00, 9.71MB/s]\n","[INFO|tokenization_utils_base.py:2026] 2024-01-05 09:01:31,104 \u003e\u003e loading file qwen.tiktoken from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat-Int8/snapshots/2fb5c225fe592d8d891cbf6c1ed5924cabe48a18/qwen.tiktoken\n","[INFO|tokenization_utils_base.py:2026] 2024-01-05 09:01:31,104 \u003e\u003e loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2026] 2024-01-05 09:01:31,104 \u003e\u003e loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2026] 2024-01-05 09:01:31,104 \u003e\u003e loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat-Int8/snapshots/2fb5c225fe592d8d891cbf6c1ed5924cabe48a18/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2026] 2024-01-05 09:01:31,104 \u003e\u003e loading file tokenizer.json from cache at None\n","config.json: 100% 1.20k/1.20k [00:00\u003c00:00, 4.84MB/s]\n","[INFO|configuration_utils.py:739] 2024-01-05 09:01:31,914 \u003e\u003e loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat-Int8/snapshots/2fb5c225fe592d8d891cbf6c1ed5924cabe48a18/config.json\n","configuration_qwen.py: 100% 2.35k/2.35k [00:00\u003c00:00, 10.9MB/s]\n","[INFO|configuration_utils.py:739] 2024-01-05 09:01:32,190 \u003e\u003e loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat-Int8/snapshots/2fb5c225fe592d8d891cbf6c1ed5924cabe48a18/config.json\n","[INFO|configuration_utils.py:802] 2024-01-05 09:01:32,191 \u003e\u003e Model config QWenConfig {\n","  \"_name_or_path\": \"Qwen/Qwen-1_8B-Chat-Int8\",\n","  \"architectures\": [\n","    \"QWenLMHeadModel\"\n","  ],\n","  \"attn_dropout_prob\": 0.0,\n","  \"auto_map\": {\n","    \"AutoConfig\": \"Qwen/Qwen-1_8B-Chat-Int8--configuration_qwen.QWenConfig\",\n","    \"AutoModelForCausalLM\": \"Qwen/Qwen-1_8B-Chat-Int8--modeling_qwen.QWenLMHeadModel\"\n","  },\n","  \"bf16\": false,\n","  \"emb_dropout_prob\": 0.0,\n","  \"fp16\": true,\n","  \"fp32\": false,\n","  \"hidden_size\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"kv_channels\": 128,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"max_position_embeddings\": 8192,\n","  \"model_type\": \"qwen\",\n","  \"no_bias\": true,\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"onnx_safe\": null,\n","  \"quantization_config\": {\n","    \"bits\": 8,\n","    \"damp_percent\": 0.01,\n","    \"desc_act\": false,\n","    \"group_size\": 128,\n","    \"model_file_base_name\": \"model\",\n","    \"model_name_or_path\": null,\n","    \"quant_method\": \"gptq\",\n","    \"static_groups\": false,\n","    \"sym\": true,\n","    \"true_sequential\": true\n","  },\n","  \"rotary_emb_base\": 10000,\n","  \"rotary_pct\": 1.0,\n","  \"scale_attn_weights\": true,\n","  \"seq_length\": 8192,\n","  \"softmax_in_fp32\": false,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"QWenTokenizer\",\n","  \"transformers_version\": \"4.36.2\",\n","  \"use_cache\": true,\n","  \"use_cache_kernel\": false,\n","  \"use_cache_quantization\": false,\n","  \"use_dynamic_ntk\": true,\n","  \"use_flash_attn\": \"auto\",\n","  \"use_logn_attn\": true,\n","  \"vocab_size\": 151936\n","}\n","\n","01/05/2024 09:01:32 - INFO - llmtuner.model.patcher - Loading 8-bit pre-quantized model.\n","modeling_qwen.py: 100% 55.6k/55.6k [00:00\u003c00:00, 113MB/s]\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/web_demo.py\", line 11, in \u003cmodule\u003e\n","    main()\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/web_demo.py\", line 5, in main\n","    demo = create_web_demo()\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/webui/interface.py\", line 59, in create_web_demo\n","    engine = Engine(pure_chat=True)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/webui/engine.py\", line 20, in __init__\n","    self.chatter = WebChatModel(manager=self.manager, demo_mode=demo_mode, lazy_init=(not pure_chat))\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/webui/chatter.py\", line 30, in __init__\n","    super().__init__()\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/chat/chat_model.py\", line 27, in __init__\n","    self.model, self.tokenizer = load_model_and_tokenizer(\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/model/loader.py\", line 87, in load_model_and_tokenizer\n","    model = AutoModelForCausalLM.from_pretrained(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 553, in from_pretrained\n","    model_class = get_class_from_dynamic_module(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py\", line 488, in get_class_from_dynamic_module\n","    final_module = get_cached_module_file(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py\", line 315, in get_cached_module_file\n","    modules_needed = check_imports(resolved_module_file)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py\", line 180, in check_imports\n","    raise ImportError(\n","ImportError: This modeling file requires the following packages that were not found in your environment: einops, transformers_stream_generator. Run `pip install einops transformers_stream_generator`\n"]}],"source":["!python src/web_demo.py \\\n","    --model_name_or_path Qwen/Qwen-1_8B-Chat-Int8 \\\n","    --adapter_name_or_path saves/Qwen-1.8B-int8-Chat/lora/demo2 \\\n","    --template default \\\n","    --finetuning_type lora"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11353,"status":"ok","timestamp":1704453747649,"user":{"displayName":"王宁","userId":"09376064211353228652"},"user_tz":-480},"id":"u0DFByEELVRw","outputId":"45d84af4-62fc-4729-8b1a-8430027f5e0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-01-05 11:22:20.591294: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-05 11:22:20.591339: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-05 11:22:20.592699: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-05 11:22:21.782359: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/train_bash.py\", line 14, in \u003cmodule\u003e\n","    main()\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/train_bash.py\", line 5, in main\n","    run_exp()\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 20, in run_exp\n","    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/model/parser.py\", line 110, in get_train_args\n","    data_args.init_for_training(training_args.seed)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/hparams/data_args.py\", line 147, in init_for_training\n","    raise ValueError(\"Undefined dataset {} in {}.\".format(name, DATA_CONFIG))\n","ValueError: Undefined dataset community in dataset_info.json.\n"]}],"source":["# !CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n","#     --stage sft \\\n","#     --do_train \\\n","#     --model_name_or_path ../Qwen-1_8B-Chat-Int8/ \\\n","#     --dataset data/community \\\n","#     --template default \\\n","#     --cutoff_len 1024 \\\n","#     --finetuning_type lora \\\n","#     --lora_target c_attn \\\n","#     --output_dir ../sft_checkpoint/ \\\n","#     --overwrite_cache \\\n","#     --per_device_train_batch_size 4 \\\n","#     --gradient_accumulation_steps 4 \\\n","#     --lr_scheduler_type cosine \\\n","#     --logging_steps 10 \\\n","#     --save_steps 1000 \\\n","#     --learning_rate 5e-5 \\\n","#     --num_train_epochs 3.0 \\\n","#     --max_samples 2000 \\\n","#     --plot_loss True\\\n","#     --fp16 True\n","\n","!CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n","  --stage sft \\\n","  --do_train True \\\n","  --model_name_or_path /content/drive/MyDrive/PyProjects/LLMs/Qwen-1_8B-Chat-Int8 \\\n","  --finetuning_type lora \\\n","  --template qwen \\\n","  --dataset_dir /content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/data \\\n","  --dataset community \\\n","  --cutoff_len 1024 \\\n","  --learning_rate 5e-05 \\\n","  --num_train_epochs 3.0 \\\n","  --max_samples 2000 \\\n","  --per_device_train_batch_size 4 \\\n","  --gradient_accumulation_steps 4 \\\n","  --lr_scheduler_type cosine \\\n","  --max_grad_norm 1.0 \\\n","  --logging_steps 10 \\\n","  --save_steps 100 \\\n","  --warmup_steps 0 \\\n","  --neftune_noise_alpha 0 \\\n","  --lora_rank 8 \\\n","  --lora_dropout 0.1 \\\n","  --lora_target c_attn \\\n","  --output_dir saves/Qwen-1.8B-int8-Chat/lora/communist \\\n","  --fp16 True \\\n","  --plot_loss True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0fMVfmwXeGL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12238,"status":"ok","timestamp":1704718753070,"user":{"displayName":"王宁","userId":"09376064211353228652"},"user_tz":-480},"id":"KXYNCkk0RWYq","outputId":"1374e29b-703f-436f-e11f-66a0020a1881"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers_stream_generator\n","  Downloading transformers-stream-generator-0.0.4.tar.gz (12 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers\u003e=4.26.1 in /usr/local/lib/python3.10/dist-packages (from transformers_stream_generator) (4.36.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.26.1-\u003etransformers_stream_generator) (3.13.1)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.26.1-\u003etransformers_stream_generator) (0.20.1)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.26.1-\u003etransformers_stream_generator) (1.23.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.26.1-\u003etransformers_stream_generator) (23.2)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.26.1-\u003etransformers_stream_generator) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.26.1-\u003etransformers_stream_generator) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.26.1-\u003etransformers_stream_generator) (2.31.0)\n","Requirement already satisfied: tokenizers\u003c0.19,\u003e=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.26.1-\u003etransformers_stream_generator) (0.15.0)\n","Requirement already satisfied: safetensors\u003e=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.26.1-\u003etransformers_stream_generator) (0.4.1)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.26.1-\u003etransformers_stream_generator) (4.66.1)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.19.3-\u003etransformers\u003e=4.26.1-\u003etransformers_stream_generator) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.19.3-\u003etransformers\u003e=4.26.1-\u003etransformers_stream_generator) (4.9.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers\u003e=4.26.1-\u003etransformers_stream_generator) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers\u003e=4.26.1-\u003etransformers_stream_generator) (3.6)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers\u003e=4.26.1-\u003etransformers_stream_generator) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers\u003e=4.26.1-\u003etransformers_stream_generator) (2023.11.17)\n","Building wheels for collected packages: transformers_stream_generator\n","  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.4-py3-none-any.whl size=12316 sha256=c94c7ad44c8c1ca5be7222822e22e0f7cf0c926f852f3c3c96665a6a7366ee96\n","  Stored in directory: /root/.cache/pip/wheels/47/1d/3c/92d88493ed40c0d9be60a391eb76c9a56e9f9b7542cb789401\n","Successfully built transformers_stream_generator\n","Installing collected packages: einops, transformers_stream_generator\n","Successfully installed einops-0.7.0 transformers_stream_generator-0.0.4\n","Collecting auto-gptq\n","  Downloading auto_gptq-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting optimum\n","  Downloading optimum-1.16.1-py3-none-any.whl (403 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m403.3/403.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: accelerate\u003e=0.22.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.25.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.16.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.1.99)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.23.5)\n","Collecting rouge (from auto-gptq)\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Collecting gekko (from auto-gptq)\n","  Downloading gekko-1.0.6-py3-none-any.whl (12.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch\u003e=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.1.0+cu121)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.1)\n","Requirement already satisfied: transformers\u003e=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.36.2)\n","Requirement already satisfied: peft\u003e=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.7.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.66.1)\n","Collecting coloredlogs (from optimum)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum) (23.2)\n","Requirement already satisfied: huggingface-hub\u003e=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum) (0.20.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate\u003e=0.22.0-\u003eauto-gptq) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate\u003e=0.22.0-\u003eauto-gptq) (6.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.8.0-\u003eoptimum) (3.13.1)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.8.0-\u003eoptimum) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.8.0-\u003eoptimum) (2.31.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.8.0-\u003eoptimum) (4.9.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003eauto-gptq) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003eauto-gptq) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13.0-\u003eauto-gptq) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.31.0-\u003eauto-gptq) (2023.6.3)\n","Requirement already satisfied: tokenizers\u003c0.19,\u003e=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.31.0-\u003eauto-gptq) (0.15.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers\u003e=4.31.0-\u003eauto-gptq) (3.20.3)\n","Collecting humanfriendly\u003e=9.1 (from coloredlogs-\u003eoptimum)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow\u003e=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets-\u003eauto-gptq) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets-\u003eauto-gptq) (0.6)\n","Requirement already satisfied: dill\u003c0.3.8,\u003e=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets-\u003eauto-gptq) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets-\u003eauto-gptq) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets-\u003eauto-gptq) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets-\u003eauto-gptq) (0.70.15)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets-\u003eauto-gptq) (3.9.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-\u003eauto-gptq) (1.16.0)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003eoptimum) (1.3.0)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets-\u003eauto-gptq) (23.1.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets-\u003eauto-gptq) (6.0.4)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets-\u003eauto-gptq) (1.9.4)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets-\u003eauto-gptq) (1.4.1)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets-\u003eauto-gptq) (1.3.1)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets-\u003eauto-gptq) (4.0.3)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.8.0-\u003eoptimum) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.8.0-\u003eoptimum) (3.6)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.8.0-\u003eoptimum) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.8.0-\u003eoptimum) (2023.11.17)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.13.0-\u003eauto-gptq) (2.1.3)\n","Requirement already satisfied: python-dateutil\u003e=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets-\u003eauto-gptq) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets-\u003eauto-gptq) (2023.3.post1)\n","Installing collected packages: rouge, humanfriendly, gekko, coloredlogs, optimum, auto-gptq\n","Successfully installed auto-gptq-0.6.0 coloredlogs-15.0.1 gekko-1.0.6 humanfriendly-10.0 optimum-1.16.1 rouge-1.0.1\n"]}],"source":["#!pip install transformers==4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed\n","!pip install transformers_stream_generator einops\n","!pip install auto-gptq optimum"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zm8f5GFeOZzY","outputId":"6b6d34fa-7827-4c36-d46d-be8c4036476b"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-01-08 13:21:19.867854: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-08 13:21:19.867908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-08 13:21:19.869220: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-08 13:21:21.033867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Running on local URL:  http://0.0.0.0:7860\n","Running on public URL: https://1a80b598408e610515.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","01/08/2024 13:23:09 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n","[INFO|training_args.py:1838] 2024-01-08 13:23:09,310 \u003e\u003e PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1751: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n","  warnings.warn(\n","01/08/2024 13:23:09 - INFO - llmtuner.model.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n","  distributed training: True, compute dtype: torch.float16\n","01/08/2024 13:23:09 - INFO - llmtuner.model.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=False,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=True,\n","dispatch_batches=None,\n","do_eval=False,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=True,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=4,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=saves/BLOOM-560M/lora/bloom-chat/runs/Jan08_13-23-09_ae7af03a4dc3,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=5,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=cosine,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=0,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=saves/BLOOM-560M/lora/bloom-chat,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=4,\n","predict_with_generate=False,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=saves/BLOOM-560M/lora/bloom-chat,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=100,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","01/08/2024 13:23:09 - INFO - llmtuner.data.loader - Loading dataset alpaca_gpt4_data_zh.json...\n","Using custom data configuration default-392957df241a898d\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-392957df241a898d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-392957df241a898d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-392957df241a898d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n","01/08/2024 13:23:09 - INFO - llmtuner.data.loader - Loading dataset oaast_sft_zh.json...\n","Using custom data configuration default-7e0d86a12a2fa780\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n","Some of the datasets have disparate format. Resetting the format of the concatenated dataset.\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:23:10,443 \u003e\u003e loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/tokenizer.json\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:23:10,444 \u003e\u003e loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:23:10,444 \u003e\u003e loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:23:10,444 \u003e\u003e loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/tokenizer_config.json\n","[INFO|configuration_utils.py:739] 2024-01-08 13:23:11,736 \u003e\u003e loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/config.json\n","[INFO|configuration_utils.py:802] 2024-01-08 13:23:11,743 \u003e\u003e Model config BloomConfig {\n","  \"_name_or_path\": \"bigscience/bloom-560m\",\n","  \"apply_residual_connection_post_layernorm\": false,\n","  \"architectures\": [\n","    \"BloomForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"attention_softmax_in_fp32\": true,\n","  \"bias_dropout_fusion\": true,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"masked_softmax_fusion\": true,\n","  \"model_type\": \"bloom\",\n","  \"n_head\": 16,\n","  \"n_inner\": null,\n","  \"n_layer\": 24,\n","  \"offset_alibi\": 100,\n","  \"pad_token_id\": 3,\n","  \"pretraining_tp\": 1,\n","  \"skip_bias_add\": true,\n","  \"skip_bias_add_qkv\": false,\n","  \"slow_but_exact\": false,\n","  \"transformers_version\": \"4.36.2\",\n","  \"unk_token_id\": 0,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250880\n","}\n","\n","[INFO|modeling_utils.py:3344] 2024-01-08 13:23:11,778 \u003e\u003e loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/model.safetensors\n","[INFO|modeling_utils.py:1341] 2024-01-08 13:23:11,793 \u003e\u003e Instantiating BloomForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:826] 2024-01-08 13:23:11,796 \u003e\u003e Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"pad_token_id\": 3\n","}\n","\n","[INFO|modeling_utils.py:4185] 2024-01-08 13:23:12,239 \u003e\u003e All model checkpoint weights were used when initializing BloomForCausalLM.\n","\n","[INFO|modeling_utils.py:4193] 2024-01-08 13:23:12,240 \u003e\u003e All the weights of BloomForCausalLM were initialized from the model checkpoint at bigscience/bloom-560m.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BloomForCausalLM for predictions without further training.\n","[INFO|modeling_utils.py:3751] 2024-01-08 13:23:12,333 \u003e\u003e Generation config file not found, using a generation config created from the model config.\n","01/08/2024 13:23:12 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n","01/08/2024 13:23:12 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n","01/08/2024 13:23:13 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/BLOOM-560M/lora/bloom_assist\n","01/08/2024 13:23:13 - INFO - llmtuner.model.loader - trainable params: 786432 || all params: 560001024 || trainable%: 0.1404\n","Running tokenizer on dataset:   0% 0/1689 [00:00\u003c?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-392957df241a898d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6cf506109d72eb59.arrow\n","Running tokenizer on dataset: 100% 1689/1689 [00:01\u003c00:00, 1026.76 examples/s]\n","input_ids:\n","[1, 36, 44799, 5299, 267, 99579, 5579, 530, 660, 48763, 64225, 103800, 17, 1387, 103800, 19502, 66799, 15, 53180, 15, 530, 214804, 41259, 427, 368, 88331, 11732, 17, 189, 114330, 29, 210, 17388, 126563, 26788, 50670, 814, 9096, 61339, 29, 10967, 644, 17388, 126563, 26788, 50670, 1533, 603, 20, 17, 210, 17388, 24325, 6583, 420, 25281, 3402, 51354, 24325, 13977, 355, 1947, 159830, 594, 170623, 2140, 63477, 355, 1409, 11392, 205591, 15224, 355, 36008, 61831, 21751, 355, 2342, 39174, 15918, 95068, 420, 603, 21, 17, 210, 92645, 51437, 420, 25281, 52604, 93146, 373, 72411, 594, 68216, 594, 1952, 15822, 2406, 745, 57008, 66652, 122793, 62809, 22528, 355, 22523, 1985, 17237, 594, 1985, 57008, 745, 50705, 32648, 355, 909, 17388, 126563, 51437, 38154, 420, 603, 22, 17, 210, 76117, 67538, 420, 76117, 1616, 53212, 15224, 82885, 355, 173024, 25281, 2833, 22249, 217562, 210, 240039, 76117, 420, 53786, 76117, 39174, 62050, 35902, 355, 11392, 24325, 20443, 355, 2342, 12937, 8745, 71268, 57941, 2043, 420, 2]\n","inputs:\n","\u003cs\u003eA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n","Human: 保持健康的三个提示。\n","Assistant:以下是保持健康的三个提示：\n","\n","1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n","\n","2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n","\n","3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。\u003c/s\u003e\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 10967, 644, 17388, 126563, 26788, 50670, 1533, 603, 20, 17, 210, 17388, 24325, 6583, 420, 25281, 3402, 51354, 24325, 13977, 355, 1947, 159830, 594, 170623, 2140, 63477, 355, 1409, 11392, 205591, 15224, 355, 36008, 61831, 21751, 355, 2342, 39174, 15918, 95068, 420, 603, 21, 17, 210, 92645, 51437, 420, 25281, 52604, 93146, 373, 72411, 594, 68216, 594, 1952, 15822, 2406, 745, 57008, 66652, 122793, 62809, 22528, 355, 22523, 1985, 17237, 594, 1985, 57008, 745, 50705, 32648, 355, 909, 17388, 126563, 51437, 38154, 420, 603, 22, 17, 210, 76117, 67538, 420, 76117, 1616, 53212, 15224, 82885, 355, 173024, 25281, 2833, 22249, 217562, 210, 240039, 76117, 420, 53786, 76117, 39174, 62050, 35902, 355, 11392, 24325, 20443, 355, 2342, 12937, 8745, 71268, 57941, 2043, 420, 2]\n","labels:\n","以下是保持健康的三个提示：\n","\n","1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n","\n","2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n","\n","3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。\u003c/s\u003e\n","[INFO|training_args.py:1838] 2024-01-08 13:23:15,282 \u003e\u003e PyTorch: setting up devices\n","[INFO|trainer.py:568] 2024-01-08 13:23:15,640 \u003e\u003e Using auto half precision backend\n","[INFO|trainer.py:1706] 2024-01-08 13:23:15,956 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1707] 2024-01-08 13:23:15,956 \u003e\u003e   Num examples = 1,689\n","[INFO|trainer.py:1708] 2024-01-08 13:23:15,956 \u003e\u003e   Num Epochs = 3\n","[INFO|trainer.py:1709] 2024-01-08 13:23:15,956 \u003e\u003e   Instantaneous batch size per device = 4\n","[INFO|trainer.py:1712] 2024-01-08 13:23:15,956 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 16\n","[INFO|trainer.py:1713] 2024-01-08 13:23:15,956 \u003e\u003e   Gradient Accumulation steps = 4\n","[INFO|trainer.py:1714] 2024-01-08 13:23:15,956 \u003e\u003e   Total optimization steps = 315\n","[INFO|trainer.py:1715] 2024-01-08 13:23:15,957 \u003e\u003e   Number of trainable parameters = 786,432\n","[WARNING|logging.py:314] 2024-01-08 13:23:15,974 \u003e\u003e You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","01/08/2024 13:23:31 - INFO - llmtuner.extras.callbacks - {'loss': 2.6925, 'learning_rate': 4.9969e-05, 'epoch': 0.05}\n","{'loss': 2.6925, 'learning_rate': 4.996892303047306e-05, 'epoch': 0.05}\n","Exception in thread Thread-6 (run_exp):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 26, in run_exp\n","    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/train/sft/workflow.py\", line 71, in run_sft\n","    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1537, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1854, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2735, in training_step\n","    loss = self.compute_loss(model, inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2758, in compute_loss\n","    outputs = model(**inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 680, in forward\n","    return model_forward(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 668, in __call__\n","    return convert_to_fp32(self.model_forward(*args, **kwargs))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 16, in decorate_autocast\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1073, in forward\n","    return self.base_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 103, in forward\n","    return self.model.forward(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py\", line 883, in forward\n","    loss = loss_fct(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\", line 1179, in forward\n","    return F.cross_entropy(input, target, weight=self.weight,\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3053, in cross_entropy\n","    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.83 GiB. GPU 0 has a total capacty of 14.75 GiB of which 2.20 GiB is free. Process 77965 has 12.55 GiB memory in use. Of the allocated memory 9.16 GiB is allocated by PyTorch, and 3.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","[INFO|training_args.py:1838] 2024-01-08 13:24:10,010 \u003e\u003e PyTorch: setting up devices\n","[INFO|training_args.py:1576] 2024-01-08 13:24:10,010 \u003e\u003e The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","01/08/2024 13:24:10 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n","[INFO|training_args.py:1838] 2024-01-08 13:24:10,015 \u003e\u003e PyTorch: setting up devices\n","Exception in thread Thread-9 (run_exp):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 20, in run_exp\n","    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/model/parser.py\", line 185, in get_train_args\n","    raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n","ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n","[INFO|training_args.py:1838] 2024-01-08 13:24:58,103 \u003e\u003e PyTorch: setting up devices\n","[INFO|training_args.py:1576] 2024-01-08 13:24:58,104 \u003e\u003e The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","01/08/2024 13:24:58 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n","[INFO|training_args.py:1838] 2024-01-08 13:24:58,109 \u003e\u003e PyTorch: setting up devices\n","01/08/2024 13:24:58 - INFO - llmtuner.model.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n","  distributed training: True, compute dtype: torch.float16\n","01/08/2024 13:24:58 - INFO - llmtuner.model.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=False,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=True,\n","dispatch_batches=None,\n","do_eval=False,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=True,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=4,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=saves/BLOOM-560M/lora/bloom-chat/runs/Jan08_13-24-58_ae7af03a4dc3,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=5,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=cosine,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=0,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=saves/BLOOM-560M/lora/bloom-chat,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=4,\n","predict_with_generate=False,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=saves/BLOOM-560M/lora/bloom-chat,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=100,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","01/08/2024 13:24:58 - INFO - llmtuner.data.loader - Loading dataset oaast_sft_zh.json...\n","Using custom data configuration default-7e0d86a12a2fa780\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:24:58,810 \u003e\u003e loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/tokenizer.json\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:24:58,811 \u003e\u003e loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:24:58,811 \u003e\u003e loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:24:58,811 \u003e\u003e loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/tokenizer_config.json\n","[INFO|configuration_utils.py:739] 2024-01-08 13:24:59,848 \u003e\u003e loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/config.json\n","[INFO|configuration_utils.py:802] 2024-01-08 13:24:59,849 \u003e\u003e Model config BloomConfig {\n","  \"_name_or_path\": \"bigscience/bloom-560m\",\n","  \"apply_residual_connection_post_layernorm\": false,\n","  \"architectures\": [\n","    \"BloomForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"attention_softmax_in_fp32\": true,\n","  \"bias_dropout_fusion\": true,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"masked_softmax_fusion\": true,\n","  \"model_type\": \"bloom\",\n","  \"n_head\": 16,\n","  \"n_inner\": null,\n","  \"n_layer\": 24,\n","  \"offset_alibi\": 100,\n","  \"pad_token_id\": 3,\n","  \"pretraining_tp\": 1,\n","  \"skip_bias_add\": true,\n","  \"skip_bias_add_qkv\": false,\n","  \"slow_but_exact\": false,\n","  \"transformers_version\": \"4.36.2\",\n","  \"unk_token_id\": 0,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250880\n","}\n","\n","[INFO|modeling_utils.py:3344] 2024-01-08 13:24:59,851 \u003e\u003e loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/model.safetensors\n","[INFO|modeling_utils.py:1341] 2024-01-08 13:24:59,861 \u003e\u003e Instantiating BloomForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:826] 2024-01-08 13:24:59,862 \u003e\u003e Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"pad_token_id\": 3\n","}\n","\n","[INFO|modeling_utils.py:4185] 2024-01-08 13:25:00,221 \u003e\u003e All model checkpoint weights were used when initializing BloomForCausalLM.\n","\n","[INFO|modeling_utils.py:4193] 2024-01-08 13:25:00,221 \u003e\u003e All the weights of BloomForCausalLM were initialized from the model checkpoint at bigscience/bloom-560m.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BloomForCausalLM for predictions without further training.\n","[INFO|modeling_utils.py:3751] 2024-01-08 13:25:00,315 \u003e\u003e Generation config file not found, using a generation config created from the model config.\n","01/08/2024 13:25:00 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n","01/08/2024 13:25:00 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n","01/08/2024 13:25:00 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/BLOOM-560M/lora/bloom_assist\n","01/08/2024 13:25:00 - INFO - llmtuner.model.loader - trainable params: 786432 || all params: 560001024 || trainable%: 0.1404\n","Running tokenizer on dataset:   0% 0/689 [00:00\u003c?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d6caef44e7fba2af.arrow\n","Running tokenizer on dataset: 100% 689/689 [00:00\u003c00:00, 798.96 examples/s]\n","input_ids:\n","[1, 36, 44799, 5299, 267, 99579, 5579, 530, 660, 48763, 64225, 103800, 17, 1387, 103800, 19502, 66799, 15, 53180, 15, 530, 214804, 41259, 427, 368, 88331, 11732, 17, 189, 114330, 29, 190219, 8281, 22570, 97073, 24438, 123256, 119645, 139958, 373, 49076, 644, 2498, 189, 9096, 61339, 29, 7136, 183933, 9195, 3435, 8281, 22570, 63953, 89782, 598, 350, 47193, 1311, 355, 30401, 2993, 4845, 124564, 219874, 9344, 1312, 11695, 36, 2898, 1311, 355, 2832, 38538, 190692, 594, 9130, 24651, 594, 219874, 420, 169288, 15724, 2279, 12893, 594, 17318, 141420, 355, 745, 4845, 210741, 1963, 89257, 17392, 814, 1616, 24438, 123256, 119645, 139958, 63953, 10098, 364, 6217, 636, 4323, 594, 8645, 76338, 155655, 594, 73996, 1533, 6277, 6277, 1311, 355, 93470, 24438, 123256, 63700, 139958, 594, 79598, 8967, 5360, 12893, 355, 30401, 13389, 15724, 60303, 1961, 189901, 373, 30558, 814, 9618, 41182, 19397, 9344, 355, 78995, 2862, 27710, 183933, 336, 27710, 78511, 1533, 189, 3435, 8281, 22570, 1533, 14524, 18006, 17, 60584, 17, 75489, 8, 40, 15459, 37, 20579, 9023, 8, 40, 27448, 37, 25569, 28, 37, 8, 40, 20579, 220887, 8, 4351, 189, 1616, 24438, 123256, 119645, 139958, 1533, 14524, 18006, 17, 60584, 17, 10474, 18006, 7143, 703, 190975, 40, 15459, 25720, 8, 37, 24646, 40, 25569, 37, 24646, 171801, 40, 24646, 162758, 37, 36433, 40, 27448, 37, 36433, 36, 20579, 40, 15459, 28, 41, 8, 15944, 8, 40, 24646, 162758, 28, 36, 2]\n","inputs:\n","\u003cs\u003eA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n","Human: 布洛芬和对乙酰氨基酚的区别是？\n","Assistant:根据维基百科，\n","布洛芬（英语：Ibuprofen），是一种非类固醇消炎药（NSAID），常用来止痛、退烧、消炎。可用于治疗经痛、偏头痛，和类风湿性关节炎。\n","对乙酰氨基酚（英语：Acetaminophen、Paracetamol、简称：APAP），又称乙酰胺酚、扑热息痛，是一种用于治疗疼痛与发烧的药物。\n","这是两种不同的药，详情可以参考维基百科.\n","参考链接：\n","布洛芬：https://zh.wikipedia.org/wiki/%E5%B8%83%E6%B4%9B%E8%8A%AC\n","对乙酰氨基酚：https://zh.wikipedia.org/zh-hans/%E5%AF%B9%E4%B9%99%E9%85%B0%E6%B0%A8%E5%9F%BA%E9%85%9A\u003c/s\u003e\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 7136, 183933, 9195, 3435, 8281, 22570, 63953, 89782, 598, 350, 47193, 1311, 355, 30401, 2993, 4845, 124564, 219874, 9344, 1312, 11695, 36, 2898, 1311, 355, 2832, 38538, 190692, 594, 9130, 24651, 594, 219874, 420, 169288, 15724, 2279, 12893, 594, 17318, 141420, 355, 745, 4845, 210741, 1963, 89257, 17392, 814, 1616, 24438, 123256, 119645, 139958, 63953, 10098, 364, 6217, 636, 4323, 594, 8645, 76338, 155655, 594, 73996, 1533, 6277, 6277, 1311, 355, 93470, 24438, 123256, 63700, 139958, 594, 79598, 8967, 5360, 12893, 355, 30401, 13389, 15724, 60303, 1961, 189901, 373, 30558, 814, 9618, 41182, 19397, 9344, 355, 78995, 2862, 27710, 183933, 336, 27710, 78511, 1533, 189, 3435, 8281, 22570, 1533, 14524, 18006, 17, 60584, 17, 75489, 8, 40, 15459, 37, 20579, 9023, 8, 40, 27448, 37, 25569, 28, 37, 8, 40, 20579, 220887, 8, 4351, 189, 1616, 24438, 123256, 119645, 139958, 1533, 14524, 18006, 17, 60584, 17, 10474, 18006, 7143, 703, 190975, 40, 15459, 25720, 8, 37, 24646, 40, 25569, 37, 24646, 171801, 40, 24646, 162758, 37, 36433, 40, 27448, 37, 36433, 36, 20579, 40, 15459, 28, 41, 8, 15944, 8, 40, 24646, 162758, 28, 36, 2]\n","labels:\n","根据维基百科，\n","布洛芬（英语：Ibuprofen），是一种非类固醇消炎药（NSAID），常用来止痛、退烧、消炎。可用于治疗经痛、偏头痛，和类风湿性关节炎。\n","对乙酰氨基酚（英语：Acetaminophen、Paracetamol、简称：APAP），又称乙酰胺酚、扑热息痛，是一种用于治疗疼痛与发烧的药物。\n","这是两种不同的药，详情可以参考维基百科.\n","参考链接：\n","布洛芬：https://zh.wikipedia.org/wiki/%E5%B8%83%E6%B4%9B%E8%8A%AC\n","对乙酰氨基酚：https://zh.wikipedia.org/zh-hans/%E5%AF%B9%E4%B9%99%E9%85%B0%E6%B0%A8%E5%9F%BA%E9%85%9A\u003c/s\u003e\n","[INFO|training_args.py:1838] 2024-01-08 13:25:01,481 \u003e\u003e PyTorch: setting up devices\n","[INFO|trainer.py:568] 2024-01-08 13:25:01,839 \u003e\u003e Using auto half precision backend\n","[INFO|trainer.py:1706] 2024-01-08 13:25:02,150 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1707] 2024-01-08 13:25:02,150 \u003e\u003e   Num examples = 689\n","[INFO|trainer.py:1708] 2024-01-08 13:25:02,150 \u003e\u003e   Num Epochs = 3\n","[INFO|trainer.py:1709] 2024-01-08 13:25:02,150 \u003e\u003e   Instantaneous batch size per device = 4\n","[INFO|trainer.py:1712] 2024-01-08 13:25:02,150 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 16\n","[INFO|trainer.py:1713] 2024-01-08 13:25:02,150 \u003e\u003e   Gradient Accumulation steps = 4\n","[INFO|trainer.py:1714] 2024-01-08 13:25:02,150 \u003e\u003e   Total optimization steps = 129\n","[INFO|trainer.py:1715] 2024-01-08 13:25:02,151 \u003e\u003e   Number of trainable parameters = 786,432\n","[WARNING|logging.py:314] 2024-01-08 13:25:02,167 \u003e\u003e You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Exception in thread Thread-10 (run_exp):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 26, in run_exp\n","    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/train/sft/workflow.py\", line 71, in run_sft\n","    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1537, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1854, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2735, in training_step\n","    loss = self.compute_loss(model, inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2758, in compute_loss\n","    outputs = model(**inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 680, in forward\n","    return model_forward(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 668, in __call__\n","    return convert_to_fp32(self.model_forward(*args, **kwargs))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 16, in decorate_autocast\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1073, in forward\n","    return self.base_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 103, in forward\n","    return self.model.forward(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py\", line 883, in forward\n","    loss = loss_fct(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\", line 1179, in forward\n","    return F.cross_entropy(input, target, weight=self.weight,\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3053, in cross_entropy\n","    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.83 GiB. GPU 0 has a total capacty of 14.75 GiB of which 2.75 GiB is free. Process 77965 has 12.00 GiB memory in use. Of the allocated memory 9.16 GiB is allocated by PyTorch, and 2.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","[INFO|training_args.py:1838] 2024-01-08 13:26:44,191 \u003e\u003e PyTorch: setting up devices\n","[INFO|training_args.py:1576] 2024-01-08 13:26:44,191 \u003e\u003e The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","01/08/2024 13:26:44 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n","[INFO|training_args.py:1838] 2024-01-08 13:26:44,197 \u003e\u003e PyTorch: setting up devices\n","01/08/2024 13:26:44 - INFO - llmtuner.model.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n","  distributed training: True, compute dtype: torch.float16\n","01/08/2024 13:26:44 - INFO - llmtuner.model.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=False,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=True,\n","dispatch_batches=None,\n","do_eval=False,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=True,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=4,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=saves/BLOOM-560M/lora/bloom-chat/runs/Jan08_13-26-44_ae7af03a4dc3,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=5,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=cosine,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=0,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=saves/BLOOM-560M/lora/bloom-chat,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=4,\n","predict_with_generate=False,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=saves/BLOOM-560M/lora/bloom-chat,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=100,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","01/08/2024 13:26:44 - INFO - llmtuner.data.loader - Loading dataset oaast_sft_zh.json...\n","Using custom data configuration default-7e0d86a12a2fa780\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:26:44,802 \u003e\u003e loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/tokenizer.json\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:26:44,802 \u003e\u003e loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:26:44,802 \u003e\u003e loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:26:44,803 \u003e\u003e loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/tokenizer_config.json\n","[INFO|configuration_utils.py:739] 2024-01-08 13:26:45,644 \u003e\u003e loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/config.json\n","[INFO|configuration_utils.py:802] 2024-01-08 13:26:45,645 \u003e\u003e Model config BloomConfig {\n","  \"_name_or_path\": \"bigscience/bloom-560m\",\n","  \"apply_residual_connection_post_layernorm\": false,\n","  \"architectures\": [\n","    \"BloomForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"attention_softmax_in_fp32\": true,\n","  \"bias_dropout_fusion\": true,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"masked_softmax_fusion\": true,\n","  \"model_type\": \"bloom\",\n","  \"n_head\": 16,\n","  \"n_inner\": null,\n","  \"n_layer\": 24,\n","  \"offset_alibi\": 100,\n","  \"pad_token_id\": 3,\n","  \"pretraining_tp\": 1,\n","  \"skip_bias_add\": true,\n","  \"skip_bias_add_qkv\": false,\n","  \"slow_but_exact\": false,\n","  \"transformers_version\": \"4.36.2\",\n","  \"unk_token_id\": 0,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250880\n","}\n","\n","[INFO|modeling_utils.py:3344] 2024-01-08 13:26:45,647 \u003e\u003e loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/model.safetensors\n","[INFO|modeling_utils.py:1341] 2024-01-08 13:26:45,656 \u003e\u003e Instantiating BloomForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:826] 2024-01-08 13:26:45,657 \u003e\u003e Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"pad_token_id\": 3\n","}\n","\n","[INFO|modeling_utils.py:4185] 2024-01-08 13:26:45,998 \u003e\u003e All model checkpoint weights were used when initializing BloomForCausalLM.\n","\n","[INFO|modeling_utils.py:4193] 2024-01-08 13:26:45,998 \u003e\u003e All the weights of BloomForCausalLM were initialized from the model checkpoint at bigscience/bloom-560m.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BloomForCausalLM for predictions without further training.\n","[INFO|modeling_utils.py:3751] 2024-01-08 13:26:46,089 \u003e\u003e Generation config file not found, using a generation config created from the model config.\n","01/08/2024 13:26:46 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n","01/08/2024 13:26:46 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n","01/08/2024 13:26:46 - INFO - llmtuner.model.loader - trainable params: 786432 || all params: 560001024 || trainable%: 0.1404\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d6caef44e7fba2af.arrow\n","input_ids:\n","[1, 36, 44799, 5299, 267, 99579, 5579, 530, 660, 48763, 64225, 103800, 17, 1387, 103800, 19502, 66799, 15, 53180, 15, 530, 214804, 41259, 427, 368, 88331, 11732, 17, 189, 114330, 29, 190219, 8281, 22570, 97073, 24438, 123256, 119645, 139958, 373, 49076, 644, 2498, 189, 9096, 61339, 29, 7136, 183933, 9195, 3435, 8281, 22570, 63953, 89782, 598, 350, 47193, 1311, 355, 30401, 2993, 4845, 124564, 219874, 9344, 1312, 11695, 36, 2898, 1311, 355, 2832, 38538, 190692, 594, 9130, 24651, 594, 219874, 420, 169288, 15724, 2279, 12893, 594, 17318, 141420, 355, 745, 4845, 210741, 1963, 89257, 17392, 814, 1616, 24438, 123256, 119645, 139958, 63953, 10098, 364, 6217, 636, 4323, 594, 8645, 76338, 155655, 594, 73996, 1533, 6277, 6277, 1311, 355, 93470, 24438, 123256, 63700, 139958, 594, 79598, 8967, 5360, 12893, 355, 30401, 13389, 15724, 60303, 1961, 189901, 373, 30558, 814, 9618, 41182, 19397, 9344, 355, 78995, 2862, 27710, 183933, 336, 27710, 78511, 1533, 189, 3435, 8281, 22570, 1533, 14524, 18006, 17, 60584, 17, 75489, 8, 40, 15459, 37, 20579, 9023, 8, 40, 27448, 37, 25569, 28, 37, 8, 40, 20579, 220887, 8, 4351, 189, 1616, 24438, 123256, 119645, 139958, 1533, 14524, 18006, 17, 60584, 17, 10474, 18006, 7143, 703, 190975, 40, 15459, 25720, 8, 37, 24646, 40, 25569, 37, 24646, 171801, 40, 24646, 162758, 37, 36433, 40, 27448, 37, 36433, 36, 20579, 40, 15459, 28, 41, 8, 15944, 8, 40, 24646, 162758, 28, 36, 2]\n","inputs:\n","\u003cs\u003eA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n","Human: 布洛芬和对乙酰氨基酚的区别是？\n","Assistant:根据维基百科，\n","布洛芬（英语：Ibuprofen），是一种非类固醇消炎药（NSAID），常用来止痛、退烧、消炎。可用于治疗经痛、偏头痛，和类风湿性关节炎。\n","对乙酰氨基酚（英语：Acetaminophen、Paracetamol、简称：APAP），又称乙酰胺酚、扑热息痛，是一种用于治疗疼痛与发烧的药物。\n","这是两种不同的药，详情可以参考维基百科.\n","参考链接：\n","布洛芬：https://zh.wikipedia.org/wiki/%E5%B8%83%E6%B4%9B%E8%8A%AC\n","对乙酰氨基酚：https://zh.wikipedia.org/zh-hans/%E5%AF%B9%E4%B9%99%E9%85%B0%E6%B0%A8%E5%9F%BA%E9%85%9A\u003c/s\u003e\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 7136, 183933, 9195, 3435, 8281, 22570, 63953, 89782, 598, 350, 47193, 1311, 355, 30401, 2993, 4845, 124564, 219874, 9344, 1312, 11695, 36, 2898, 1311, 355, 2832, 38538, 190692, 594, 9130, 24651, 594, 219874, 420, 169288, 15724, 2279, 12893, 594, 17318, 141420, 355, 745, 4845, 210741, 1963, 89257, 17392, 814, 1616, 24438, 123256, 119645, 139958, 63953, 10098, 364, 6217, 636, 4323, 594, 8645, 76338, 155655, 594, 73996, 1533, 6277, 6277, 1311, 355, 93470, 24438, 123256, 63700, 139958, 594, 79598, 8967, 5360, 12893, 355, 30401, 13389, 15724, 60303, 1961, 189901, 373, 30558, 814, 9618, 41182, 19397, 9344, 355, 78995, 2862, 27710, 183933, 336, 27710, 78511, 1533, 189, 3435, 8281, 22570, 1533, 14524, 18006, 17, 60584, 17, 75489, 8, 40, 15459, 37, 20579, 9023, 8, 40, 27448, 37, 25569, 28, 37, 8, 40, 20579, 220887, 8, 4351, 189, 1616, 24438, 123256, 119645, 139958, 1533, 14524, 18006, 17, 60584, 17, 10474, 18006, 7143, 703, 190975, 40, 15459, 25720, 8, 37, 24646, 40, 25569, 37, 24646, 171801, 40, 24646, 162758, 37, 36433, 40, 27448, 37, 36433, 36, 20579, 40, 15459, 28, 41, 8, 15944, 8, 40, 24646, 162758, 28, 36, 2]\n","labels:\n","根据维基百科，\n","布洛芬（英语：Ibuprofen），是一种非类固醇消炎药（NSAID），常用来止痛、退烧、消炎。可用于治疗经痛、偏头痛，和类风湿性关节炎。\n","对乙酰氨基酚（英语：Acetaminophen、Paracetamol、简称：APAP），又称乙酰胺酚、扑热息痛，是一种用于治疗疼痛与发烧的药物。\n","这是两种不同的药，详情可以参考维基百科.\n","参考链接：\n","布洛芬：https://zh.wikipedia.org/wiki/%E5%B8%83%E6%B4%9B%E8%8A%AC\n","对乙酰氨基酚：https://zh.wikipedia.org/zh-hans/%E5%AF%B9%E4%B9%99%E9%85%B0%E6%B0%A8%E5%9F%BA%E9%85%9A\u003c/s\u003e\n","[INFO|training_args.py:1838] 2024-01-08 13:26:46,382 \u003e\u003e PyTorch: setting up devices\n","[INFO|trainer.py:568] 2024-01-08 13:26:46,746 \u003e\u003e Using auto half precision backend\n","[INFO|trainer.py:1706] 2024-01-08 13:26:47,061 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1707] 2024-01-08 13:26:47,061 \u003e\u003e   Num examples = 689\n","[INFO|trainer.py:1708] 2024-01-08 13:26:47,061 \u003e\u003e   Num Epochs = 3\n","[INFO|trainer.py:1709] 2024-01-08 13:26:47,061 \u003e\u003e   Instantaneous batch size per device = 4\n","[INFO|trainer.py:1712] 2024-01-08 13:26:47,061 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 16\n","[INFO|trainer.py:1713] 2024-01-08 13:26:47,061 \u003e\u003e   Gradient Accumulation steps = 4\n","[INFO|trainer.py:1714] 2024-01-08 13:26:47,061 \u003e\u003e   Total optimization steps = 129\n","[INFO|trainer.py:1715] 2024-01-08 13:26:47,063 \u003e\u003e   Number of trainable parameters = 786,432\n","[WARNING|logging.py:314] 2024-01-08 13:26:47,079 \u003e\u003e You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Exception in thread Thread-12 (run_exp):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 26, in run_exp\n","    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/train/sft/workflow.py\", line 71, in run_sft\n","    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1537, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1854, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2735, in training_step\n","    loss = self.compute_loss(model, inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2758, in compute_loss\n","    outputs = model(**inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 680, in forward\n","    return model_forward(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 668, in __call__\n","    return convert_to_fp32(self.model_forward(*args, **kwargs))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 16, in decorate_autocast\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1073, in forward\n","    return self.base_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 103, in forward\n","    return self.model.forward(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py\", line 883, in forward\n","    loss = loss_fct(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\", line 1179, in forward\n","    return F.cross_entropy(input, target, weight=self.weight,\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3053, in cross_entropy\n","    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.83 GiB. GPU 0 has a total capacty of 14.75 GiB of which 2.75 GiB is free. Process 77965 has 12.00 GiB memory in use. Of the allocated memory 9.16 GiB is allocated by PyTorch, and 2.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","[INFO|training_args.py:1838] 2024-01-08 13:27:21,073 \u003e\u003e PyTorch: setting up devices\n","[INFO|training_args.py:1576] 2024-01-08 13:27:21,073 \u003e\u003e The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","01/08/2024 13:27:21 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n","[INFO|training_args.py:1838] 2024-01-08 13:27:21,078 \u003e\u003e PyTorch: setting up devices\n","Exception in thread Thread-14 (run_exp):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 20, in run_exp\n","    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/model/parser.py\", line 185, in get_train_args\n","    raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n","ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n","[INFO|training_args.py:1838] 2024-01-08 13:27:33,842 \u003e\u003e PyTorch: setting up devices\n","[INFO|training_args.py:1576] 2024-01-08 13:27:33,843 \u003e\u003e The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","01/08/2024 13:27:33 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n","[INFO|training_args.py:1838] 2024-01-08 13:27:33,848 \u003e\u003e PyTorch: setting up devices\n","Exception in thread Thread-15 (run_exp):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 20, in run_exp\n","    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/model/parser.py\", line 185, in get_train_args\n","    raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n","ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n","[INFO|training_args.py:1838] 2024-01-08 13:27:54,109 \u003e\u003e PyTorch: setting up devices\n","[INFO|training_args.py:1576] 2024-01-08 13:27:54,110 \u003e\u003e The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","01/08/2024 13:27:54 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n","[INFO|training_args.py:1838] 2024-01-08 13:27:54,114 \u003e\u003e PyTorch: setting up devices\n","Exception in thread Thread-16 (run_exp):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 20, in run_exp\n","    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/llmtuner/model/parser.py\", line 185, in get_train_args\n","    raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n","ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n","[INFO|training_args.py:1838] 2024-01-08 13:28:29,638 \u003e\u003e PyTorch: setting up devices\n","[INFO|training_args.py:1576] 2024-01-08 13:28:29,638 \u003e\u003e The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","01/08/2024 13:28:29 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n","[INFO|training_args.py:1838] 2024-01-08 13:28:29,643 \u003e\u003e PyTorch: setting up devices\n","01/08/2024 13:28:29 - INFO - llmtuner.model.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n","  distributed training: True, compute dtype: torch.float16\n","01/08/2024 13:28:29 - INFO - llmtuner.model.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=False,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=True,\n","dispatch_batches=None,\n","do_eval=False,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=True,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=4,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=\u003cHUB_TOKEN\u003e,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=saves/BLOOM-560M/lora/bloom-chat/runs/Jan08_13-28-29_ae7af03a4dc3,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=5,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=cosine,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=0,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=saves/BLOOM-560M/lora/bloom-chat,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=4,\n","predict_with_generate=False,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=\u003cPUSH_TO_HUB_TOKEN\u003e,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=saves/BLOOM-560M/lora/bloom-chat,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=100,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","01/08/2024 13:28:29 - INFO - llmtuner.data.loader - Loading dataset oaast_sft_zh.json...\n","Using custom data configuration default-7e0d86a12a2fa780\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:28:30,248 \u003e\u003e loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/tokenizer.json\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:28:30,248 \u003e\u003e loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:28:30,248 \u003e\u003e loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:28:30,248 \u003e\u003e loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/tokenizer_config.json\n","[INFO|configuration_utils.py:739] 2024-01-08 13:28:31,130 \u003e\u003e loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/config.json\n","[INFO|configuration_utils.py:802] 2024-01-08 13:28:31,131 \u003e\u003e Model config BloomConfig {\n","  \"_name_or_path\": \"bigscience/bloom-560m\",\n","  \"apply_residual_connection_post_layernorm\": false,\n","  \"architectures\": [\n","    \"BloomForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"attention_softmax_in_fp32\": true,\n","  \"bias_dropout_fusion\": true,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"masked_softmax_fusion\": true,\n","  \"model_type\": \"bloom\",\n","  \"n_head\": 16,\n","  \"n_inner\": null,\n","  \"n_layer\": 24,\n","  \"offset_alibi\": 100,\n","  \"pad_token_id\": 3,\n","  \"pretraining_tp\": 1,\n","  \"skip_bias_add\": true,\n","  \"skip_bias_add_qkv\": false,\n","  \"slow_but_exact\": false,\n","  \"transformers_version\": \"4.36.2\",\n","  \"unk_token_id\": 0,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250880\n","}\n","\n","[INFO|modeling_utils.py:3344] 2024-01-08 13:28:31,133 \u003e\u003e loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/model.safetensors\n","[INFO|modeling_utils.py:1341] 2024-01-08 13:28:31,142 \u003e\u003e Instantiating BloomForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:826] 2024-01-08 13:28:31,143 \u003e\u003e Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"pad_token_id\": 3\n","}\n","\n","[INFO|modeling_utils.py:4185] 2024-01-08 13:28:31,493 \u003e\u003e All model checkpoint weights were used when initializing BloomForCausalLM.\n","\n","[INFO|modeling_utils.py:4193] 2024-01-08 13:28:31,493 \u003e\u003e All the weights of BloomForCausalLM were initialized from the model checkpoint at bigscience/bloom-560m.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BloomForCausalLM for predictions without further training.\n","[INFO|modeling_utils.py:3751] 2024-01-08 13:28:31,578 \u003e\u003e Generation config file not found, using a generation config created from the model config.\n","01/08/2024 13:28:31 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n","01/08/2024 13:28:31 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n","01/08/2024 13:28:31 - INFO - llmtuner.model.loader - trainable params: 786432 || all params: 560001024 || trainable%: 0.1404\n","Running tokenizer on dataset:   0% 0/500 [00:00\u003c?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7e0d86a12a2fa780/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-23006e31fe7b2877.arrow\n","Running tokenizer on dataset: 100% 500/500 [00:00\u003c00:00, 800.17 examples/s]\n","input_ids:\n","[1, 36, 44799, 5299, 267, 99579, 5579, 530, 660, 48763, 64225, 103800, 17, 1387, 103800, 19502, 66799, 15, 53180, 15, 530, 214804, 41259, 427, 368, 88331, 11732, 17, 189, 114330, 29, 190219, 8281, 22570, 97073, 24438, 123256, 119645, 139958, 373, 49076, 644, 2498, 189, 9096, 61339, 29, 7136, 183933, 9195, 3435, 8281, 22570, 63953, 89782, 598, 350, 47193, 1311, 355, 30401, 2993, 4845, 124564, 219874, 9344, 1312, 11695, 36, 2898, 1311, 355, 2832, 38538, 190692, 594, 9130, 24651, 594, 219874, 420, 169288, 15724, 2279, 12893, 594, 17318, 141420, 355, 745, 4845, 210741, 1963, 89257, 17392, 814, 1616, 24438, 123256, 119645, 139958, 63953, 10098, 364, 6217, 636, 4323, 594, 8645, 76338, 155655, 594, 73996, 1533, 6277, 6277, 1311, 355, 93470, 24438, 123256, 63700, 139958, 594, 79598, 8967, 5360, 12893, 355, 30401, 13389, 15724, 60303, 1961, 189901, 373, 30558, 814, 9618, 41182, 19397, 9344, 355, 78995, 2862, 27710, 183933, 336, 27710, 78511, 1533, 189, 3435, 8281, 22570, 1533, 14524, 18006, 17, 60584, 17, 75489, 8, 40, 15459, 37, 20579, 9023, 8, 40, 27448, 37, 25569, 28, 37, 8, 40, 20579, 220887, 8, 4351, 189, 1616, 24438, 123256, 119645, 139958, 1533, 14524, 18006, 17, 60584, 17, 10474, 18006, 7143, 703, 190975, 40, 15459, 25720, 8, 37, 24646, 40, 25569, 37, 24646, 171801, 40, 24646, 162758, 37, 36433, 40, 27448, 37, 36433, 36, 20579, 40, 15459, 28, 41, 8, 15944, 8, 40, 24646, 162758, 28, 36, 2]\n","inputs:\n","\u003cs\u003eA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n","Human: 布洛芬和对乙酰氨基酚的区别是？\n","Assistant:根据维基百科，\n","布洛芬（英语：Ibuprofen），是一种非类固醇消炎药（NSAID），常用来止痛、退烧、消炎。可用于治疗经痛、偏头痛，和类风湿性关节炎。\n","对乙酰氨基酚（英语：Acetaminophen、Paracetamol、简称：APAP），又称乙酰胺酚、扑热息痛，是一种用于治疗疼痛与发烧的药物。\n","这是两种不同的药，详情可以参考维基百科.\n","参考链接：\n","布洛芬：https://zh.wikipedia.org/wiki/%E5%B8%83%E6%B4%9B%E8%8A%AC\n","对乙酰氨基酚：https://zh.wikipedia.org/zh-hans/%E5%AF%B9%E4%B9%99%E9%85%B0%E6%B0%A8%E5%9F%BA%E9%85%9A\u003c/s\u003e\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 7136, 183933, 9195, 3435, 8281, 22570, 63953, 89782, 598, 350, 47193, 1311, 355, 30401, 2993, 4845, 124564, 219874, 9344, 1312, 11695, 36, 2898, 1311, 355, 2832, 38538, 190692, 594, 9130, 24651, 594, 219874, 420, 169288, 15724, 2279, 12893, 594, 17318, 141420, 355, 745, 4845, 210741, 1963, 89257, 17392, 814, 1616, 24438, 123256, 119645, 139958, 63953, 10098, 364, 6217, 636, 4323, 594, 8645, 76338, 155655, 594, 73996, 1533, 6277, 6277, 1311, 355, 93470, 24438, 123256, 63700, 139958, 594, 79598, 8967, 5360, 12893, 355, 30401, 13389, 15724, 60303, 1961, 189901, 373, 30558, 814, 9618, 41182, 19397, 9344, 355, 78995, 2862, 27710, 183933, 336, 27710, 78511, 1533, 189, 3435, 8281, 22570, 1533, 14524, 18006, 17, 60584, 17, 75489, 8, 40, 15459, 37, 20579, 9023, 8, 40, 27448, 37, 25569, 28, 37, 8, 40, 20579, 220887, 8, 4351, 189, 1616, 24438, 123256, 119645, 139958, 1533, 14524, 18006, 17, 60584, 17, 10474, 18006, 7143, 703, 190975, 40, 15459, 25720, 8, 37, 24646, 40, 25569, 37, 24646, 171801, 40, 24646, 162758, 37, 36433, 40, 27448, 37, 36433, 36, 20579, 40, 15459, 28, 41, 8, 15944, 8, 40, 24646, 162758, 28, 36, 2]\n","labels:\n","根据维基百科，\n","布洛芬（英语：Ibuprofen），是一种非类固醇消炎药（NSAID），常用来止痛、退烧、消炎。可用于治疗经痛、偏头痛，和类风湿性关节炎。\n","对乙酰氨基酚（英语：Acetaminophen、Paracetamol、简称：APAP），又称乙酰胺酚、扑热息痛，是一种用于治疗疼痛与发烧的药物。\n","这是两种不同的药，详情可以参考维基百科.\n","参考链接：\n","布洛芬：https://zh.wikipedia.org/wiki/%E5%B8%83%E6%B4%9B%E8%8A%AC\n","对乙酰氨基酚：https://zh.wikipedia.org/zh-hans/%E5%AF%B9%E4%B9%99%E9%85%B0%E6%B0%A8%E5%9F%BA%E9%85%9A\u003c/s\u003e\n","[INFO|training_args.py:1838] 2024-01-08 13:28:32,497 \u003e\u003e PyTorch: setting up devices\n","[INFO|trainer.py:568] 2024-01-08 13:28:32,880 \u003e\u003e Using auto half precision backend\n","[INFO|trainer.py:1706] 2024-01-08 13:28:33,188 \u003e\u003e ***** Running training *****\n","[INFO|trainer.py:1707] 2024-01-08 13:28:33,189 \u003e\u003e   Num examples = 500\n","[INFO|trainer.py:1708] 2024-01-08 13:28:33,189 \u003e\u003e   Num Epochs = 3\n","[INFO|trainer.py:1709] 2024-01-08 13:28:33,189 \u003e\u003e   Instantaneous batch size per device = 4\n","[INFO|trainer.py:1712] 2024-01-08 13:28:33,189 \u003e\u003e   Total train batch size (w. parallel, distributed \u0026 accumulation) = 16\n","[INFO|trainer.py:1713] 2024-01-08 13:28:33,189 \u003e\u003e   Gradient Accumulation steps = 4\n","[INFO|trainer.py:1714] 2024-01-08 13:28:33,189 \u003e\u003e   Total optimization steps = 93\n","[INFO|trainer.py:1715] 2024-01-08 13:28:33,190 \u003e\u003e   Number of trainable parameters = 786,432\n","[WARNING|logging.py:314] 2024-01-08 13:28:33,211 \u003e\u003e You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","01/08/2024 13:28:58 - INFO - llmtuner.extras.callbacks - {'loss': 2.8700, 'learning_rate': 4.9644e-05, 'epoch': 0.16}\n","{'loss': 2.87, 'learning_rate': 4.964424488287009e-05, 'epoch': 0.16}\n","01/08/2024 13:29:25 - INFO - llmtuner.extras.callbacks - {'loss': 2.9108, 'learning_rate': 4.8587e-05, 'epoch': 0.32}\n","{'loss': 2.9108, 'learning_rate': 4.858710446774951e-05, 'epoch': 0.32}\n","01/08/2024 13:29:56 - INFO - llmtuner.extras.callbacks - {'loss': 2.8064, 'learning_rate': 4.6859e-05, 'epoch': 0.48}\n","{'loss': 2.8064, 'learning_rate': 4.685866540361456e-05, 'epoch': 0.48}\n","01/08/2024 13:30:19 - INFO - llmtuner.extras.callbacks - {'loss': 2.8127, 'learning_rate': 4.4508e-05, 'epoch': 0.64}\n","{'loss': 2.8127, 'learning_rate': 4.45081197738023e-05, 'epoch': 0.64}\n","01/08/2024 13:30:40 - INFO - llmtuner.extras.callbacks - {'loss': 2.6789, 'learning_rate': 4.1602e-05, 'epoch': 0.80}\n","{'loss': 2.6789, 'learning_rate': 4.160236506918098e-05, 'epoch': 0.8}\n","01/08/2024 13:31:03 - INFO - llmtuner.extras.callbacks - {'loss': 2.8729, 'learning_rate': 3.8224e-05, 'epoch': 0.96}\n","{'loss': 2.8729, 'learning_rate': 3.822410025817406e-05, 'epoch': 0.96}\n","01/08/2024 13:31:25 - INFO - llmtuner.extras.callbacks - {'loss': 2.7421, 'learning_rate': 3.4469e-05, 'epoch': 1.12}\n","{'loss': 2.7421, 'learning_rate': 3.44694721402644e-05, 'epoch': 1.12}\n","01/08/2024 13:31:52 - INFO - llmtuner.extras.callbacks - {'loss': 2.7847, 'learning_rate': 3.0445e-05, 'epoch': 1.28}\n","{'loss': 2.7847, 'learning_rate': 3.0445338968721287e-05, 'epoch': 1.28}\n","01/08/2024 13:32:17 - INFO - llmtuner.extras.callbacks - {'loss': 2.7328, 'learning_rate': 2.6266e-05, 'epoch': 1.44}\n","{'loss': 2.7328, 'learning_rate': 2.6266229220967818e-05, 'epoch': 1.44}\n","01/08/2024 13:32:42 - INFO - llmtuner.extras.callbacks - {'loss': 2.9069, 'learning_rate': 2.2051e-05, 'epoch': 1.60}\n","{'loss': 2.9069, 'learning_rate': 2.2051082071228854e-05, 'epoch': 1.6}\n","01/08/2024 13:33:04 - INFO - llmtuner.extras.callbacks - {'loss': 2.7437, 'learning_rate': 1.7920e-05, 'epoch': 1.76}\n","{'loss': 2.7437, 'learning_rate': 1.79198623329424e-05, 'epoch': 1.76}\n","01/08/2024 13:33:30 - INFO - llmtuner.extras.callbacks - {'loss': 2.8489, 'learning_rate': 1.3990e-05, 'epoch': 1.92}\n","{'loss': 2.8489, 'learning_rate': 1.399014621105914e-05, 'epoch': 1.92}\n","01/08/2024 13:33:55 - INFO - llmtuner.extras.callbacks - {'loss': 2.9401, 'learning_rate': 1.0374e-05, 'epoch': 2.08}\n","{'loss': 2.9401, 'learning_rate': 1.0373775035117305e-05, 'epoch': 2.08}\n","01/08/2024 13:34:16 - INFO - llmtuner.extras.callbacks - {'loss': 2.7951, 'learning_rate': 7.1737e-06, 'epoch': 2.24}\n","{'loss': 2.7951, 'learning_rate': 7.173672209219495e-06, 'epoch': 2.24}\n","01/08/2024 13:34:44 - INFO - llmtuner.extras.callbacks - {'loss': 2.7639, 'learning_rate': 4.4809e-06, 'epoch': 2.40}\n","{'loss': 2.7639, 'learning_rate': 4.480913969818098e-06, 'epoch': 2.4}\n","01/08/2024 13:35:08 - INFO - llmtuner.extras.callbacks - {'loss': 2.8458, 'learning_rate': 2.7440e-06, 'epoch': 2.56}\n","{'loss': 2.8458, 'learning_rate': 2.7440387297912123e-06, 'epoch': 2.56}\n","01/08/2024 13:35:35 - INFO - llmtuner.extras.callbacks - {'loss': 2.8664, 'learning_rate': 1.1465e-06, 'epoch': 2.72}\n","{'loss': 2.8664, 'learning_rate': 1.1465185899987797e-06, 'epoch': 2.72}\n","01/08/2024 13:35:57 - INFO - llmtuner.extras.callbacks - {'loss': 2.7403, 'learning_rate': 2.2788e-07, 'epoch': 2.88}\n","{'loss': 2.7403, 'learning_rate': 2.27878296044029e-07, 'epoch': 2.88}\n","[INFO|trainer.py:1947] 2024-01-08 13:36:14,990 \u003e\u003e \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","01/08/2024 13:36:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 2.98}\n","{'train_runtime': 461.8003, 'train_samples_per_second': 3.248, 'train_steps_per_second': 0.201, 'train_loss': 2.8094007327992427, 'epoch': 2.98}\n","[INFO|trainer.py:2889] 2024-01-08 13:36:14,999 \u003e\u003e Saving model checkpoint to saves/BLOOM-560M/lora/bloom-chat\n","[INFO|tokenization_utils_base.py:2432] 2024-01-08 13:36:15,054 \u003e\u003e tokenizer config file saved in saves/BLOOM-560M/lora/bloom-chat/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2441] 2024-01-08 13:36:15,057 \u003e\u003e Special tokens file saved in saves/BLOOM-560M/lora/bloom-chat/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =       2.98\n","  train_loss               =     2.8094\n","  train_runtime            = 0:07:41.80\n","  train_samples_per_second =      3.248\n","  train_steps_per_second   =      0.201\n","[INFO|modelcard.py:452] 2024-01-08 13:36:15,296 \u003e\u003e Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:36:34,472 \u003e\u003e loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/tokenizer.json\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:36:34,472 \u003e\u003e loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:36:34,472 \u003e\u003e loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2026] 2024-01-08 13:36:34,472 \u003e\u003e loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/tokenizer_config.json\n","[INFO|configuration_utils.py:739] 2024-01-08 13:36:35,514 \u003e\u003e loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/config.json\n","[INFO|configuration_utils.py:802] 2024-01-08 13:36:35,515 \u003e\u003e Model config BloomConfig {\n","  \"_name_or_path\": \"bigscience/bloom-560m\",\n","  \"apply_residual_connection_post_layernorm\": false,\n","  \"architectures\": [\n","    \"BloomForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"attention_softmax_in_fp32\": true,\n","  \"bias_dropout_fusion\": true,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"masked_softmax_fusion\": true,\n","  \"model_type\": \"bloom\",\n","  \"n_head\": 16,\n","  \"n_inner\": null,\n","  \"n_layer\": 24,\n","  \"offset_alibi\": 100,\n","  \"pad_token_id\": 3,\n","  \"pretraining_tp\": 1,\n","  \"skip_bias_add\": true,\n","  \"skip_bias_add_qkv\": false,\n","  \"slow_but_exact\": false,\n","  \"transformers_version\": \"4.36.2\",\n","  \"unk_token_id\": 0,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250880\n","}\n","\n","[INFO|modeling_utils.py:3344] 2024-01-08 13:36:35,517 \u003e\u003e loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971/model.safetensors\n","[INFO|modeling_utils.py:1341] 2024-01-08 13:36:35,527 \u003e\u003e Instantiating BloomForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:826] 2024-01-08 13:36:35,529 \u003e\u003e Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"pad_token_id\": 3\n","}\n","\n","[INFO|modeling_utils.py:4185] 2024-01-08 13:36:35,864 \u003e\u003e All model checkpoint weights were used when initializing BloomForCausalLM.\n","\n","[INFO|modeling_utils.py:4193] 2024-01-08 13:36:35,864 \u003e\u003e All the weights of BloomForCausalLM were initialized from the model checkpoint at bigscience/bloom-560m.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BloomForCausalLM for predictions without further training.\n","[INFO|modeling_utils.py:3751] 2024-01-08 13:36:35,958 \u003e\u003e Generation config file not found, using a generation config created from the model config.\n","01/08/2024 13:36:35 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n","01/08/2024 13:36:36 - INFO - llmtuner.model.adapter - Merged 2 adapter(s).\n","01/08/2024 13:36:36 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/BLOOM-560M/lora/bloom_assist,saves/BLOOM-560M/lora/bloom-chat\n","01/08/2024 13:36:36 - INFO - llmtuner.model.loader - trainable params: 0 || all params: 559214592 || trainable%: 0.0000\n","01/08/2024 13:36:36 - INFO - llmtuner.model.loader - This IS expected that the trainable params is 0 if you are using model for inference only.\n","Keyboard interruption in main thread... closing server.\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2361, in block_thread\n","    time.sleep(0.1)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/train_web.py\", line 11, in \u003cmodule\u003e\n","    main()\n","  File \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/src/train_web.py\", line 7, in main\n","    demo.launch(server_name=\"0.0.0.0\", share=True, inbrowser=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2266, in launch\n","    self.block_thread()\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2363, in block_thread\n","    print(\"Keyboard interruption in main thread... closing server.\")\n","KeyboardInterrupt\n","Killing tunnel 0.0.0.0:7860 \u003c\u003e https://1a80b598408e610515.gradio.live\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0 python src/train_web.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75831,"status":"ok","timestamp":1704369959814,"user":{"displayName":"王宁","userId":"09376064211353228652"},"user_tz":-480},"id":"lfY1gAJVXF1C","outputId":"ee6bbfa6-a6f6-4e31-9ca9-37a769a55822"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'flash-attention'...\n","remote: Enumerating objects: 4590, done.\u001b[K\n","remote: Counting objects: 100% (2206/2206), done.\u001b[K\n","remote: Compressing objects: 100% (348/348), done.\u001b[K\n","remote: Total 4590 (delta 1974), reused 1891 (delta 1851), pack-reused 2384\u001b[K\n","Receiving objects: 100% (4590/4590), 6.97 MiB | 5.26 MiB/s, done.\n","Resolving deltas: 100% (3225/3225), done.\n","Updating files: 100% (406/406), done.\n","Processing /content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/flash-attention\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.4.2) (2.1.0+cu121)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.4.2) (0.7.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.4.2) (23.2)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.4.2) (1.11.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch-\u003eflash-attn==2.4.2) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch-\u003eflash-attn==2.4.2) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch-\u003eflash-attn==2.4.2) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-\u003eflash-attn==2.4.2) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-\u003eflash-attn==2.4.2) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-\u003eflash-attn==2.4.2) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch-\u003eflash-attn==2.4.2) (2.1.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch-\u003eflash-attn==2.4.2) (2.1.3)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch-\u003eflash-attn==2.4.2) (1.3.0)\n","Building wheels for collected packages: flash-attn\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─\u003e\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for flash-attn (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for flash-attn\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for flash-attn\n","Failed to build flash-attn\n","\u001b[31mERROR: Could not build wheels for flash-attn, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!git init\n","!git add .\n","!git commit -m \"上传LLMs文件夹\"\n","!git remote add origin https://github.com/Hurricane-hub/NEU_NLP.git\n","#!git push origin master"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4537,"status":"ok","timestamp":1704720062071,"user":{"displayName":"王宁","userId":"09376064211353228652"},"user_tz":-480},"id":"40c1ppRmXigF","outputId":"0540dd6c-8de7-4c79-effb-ab5fa420de83"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting flash_attn\n","  Downloading flash_attn-2.4.2.tar.gz (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m^C\n"]}],"source":["#!Get-FileHash -Path \"/content/drive/MyDrive/PyProjects/LLMs/LLaMA-Factory/data/community.json\" -Algorithm SHA1\n","!pip install flash_attn"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMiQPJE2yyD/9L/gV8q7xhb","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}