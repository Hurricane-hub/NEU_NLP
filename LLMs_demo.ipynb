{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bTyIoNND4PK",
        "outputId": "d87303f9-24e0-42f6-8ddf-f2d3096815c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 5906, done.\u001b[K\n",
            "remote: Counting objects: 100% (1783/1783), done.\u001b[K\n",
            "remote: Compressing objects: 100% (156/156), done.\u001b[K\n",
            "remote: Total 5906 (delta 1678), reused 1638 (delta 1627), pack-reused 4123\u001b[K\n",
            "Receiving objects: 100% (5906/5906), 185.92 MiB | 10.79 MiB/s, done.\n",
            "Resolving deltas: 100% (4315/4315), done.\n",
            "Updating files: 100% (132/132), done.\n"
          ]
        }
      ],
      "source": [
        "#!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqIp2-UCFozu"
      },
      "outputs": [],
      "source": [
        "%cd LLaMA-Factory\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYqU1sN9QMxM"
      },
      "outputs": [],
      "source": [
        "# !git lfs install\n",
        "# !git clone https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKJ-ioB7F0mO"
      },
      "outputs": [],
      "source": [
        "!pip install kaleido\n",
        "!pip install cohere\n",
        "!pip install openai\n",
        "!pip install fastapi\n",
        "!pip install python-multipart\n",
        "!pip install uvicorn\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o6IuR8KJg6o"
      },
      "outputs": [],
      "source": [
        "!python src/web_demo.py \\\n",
        "    --model_name_or_path Qwen/Qwen-1_8B-Chat-Int8 \\\n",
        "    --adapter_name_or_path saves/Qwen-1.8B-int8-Chat/lora/communist \\\n",
        "    --template default \\\n",
        "    --finetuning_type lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0DFByEELVRw"
      },
      "outputs": [],
      "source": [
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "  --stage sft \\\n",
        "  --do_train True \\\n",
        "  --model_name_or_path /PyProjects/LLMs/Qwen-1_8B-Chat-Int8 \\\n",
        "  --finetuning_type lora \\\n",
        "  --template qwen \\\n",
        "  --dataset_dir /PyProjects/LLMs/LLaMA-Factory/data \\\n",
        "  --dataset community,oaast_stf_zh \\\n",
        "  --cutoff_len 1024 \\\n",
        "  --learning_rate 5e-05 \\\n",
        "  --num_train_epochs 3.0 \\\n",
        "  --max_samples 1000 \\\n",
        "  --per_device_train_batch_size 4 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --lr_scheduler_type cosine \\\n",
        "  --max_grad_norm 1.0 \\\n",
        "  --logging_steps 10 \\\n",
        "  --save_steps 100 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --neftune_noise_alpha 0 \\\n",
        "  --lora_rank 8 \\\n",
        "  --lora_dropout 0.1 \\\n",
        "  --lora_target c_attn \\\n",
        "  --output_dir saves/Qwen-1.8B-int8-Chat/lora/communist \\\n",
        "  --fp16 True \\\n",
        "  --plot_loss True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXYNCkk0RWYq"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers==4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed\n",
        "!pip install transformers_stream_generator einops\n",
        "!pip install auto-gptq optimum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm8f5GFeOZzY"
      },
      "outputs": [],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python src/train_web.py\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXounKK7TMU0ht0gKikMFy"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}